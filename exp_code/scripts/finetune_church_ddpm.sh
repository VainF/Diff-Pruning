python -m torch.distributed.launch --nproc_per_node=4 --master_port 22223 --use_env finetune.py \
--config church.yml \
--timesteps 100 \
--eta 0 \
--ni \
--exp run/finetune/church_ddpm_$1_0.3_finetuned \
--doc post_training \
--skip_type uniform  \
--pruning_ratio 0.3 \
--use_ema \
--use_pretrained \
--load_pruned_model run/pruned/church_ddpm_$1_0.3.pth \